# Sistema de IngestÃ£o e Busca com RAG

Este projeto implementa um sistema completo de RAG (Retrieval-Augmented Generation) que processa PDFs, cria embeddings e armazena no PostgreSQL com pgVector usando LangChain.

## ğŸ› ï¸ ConfiguraÃ§Ã£o

### 1. DependÃªncias
```bash
# Instalar dependÃªncias Python
pip install -r requirements.txt

# Instalar dependÃªncias adicionais
pip install langchain-openai langchain-postgres psycopg[binary] psycopg-pool
```

### 2. Banco de Dados
```bash
# Iniciar PostgreSQL com pgVector
docker-compose up -d

# Verificar se estÃ¡ rodando
docker ps
```

### 3. VariÃ¡veis de Ambiente
Configure o arquivo `.env` com as configuraÃ§Ãµes necessÃ¡rias:
```env
# PDF Configuration
PDF_PATH=/caminho/para/seu/documento.pdf

# PGVector Configuration
PGVECTOR_COLLECTION=document_embeddings
PGVECTOR_URL=postgresql://postgres:postgres@localhost:5432/rag

# OpenAI Configuration (necessÃ¡ria para embeddings e LLM)
OPENAI_API_KEY=sua_chave_api_openai_aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_MODEL=gpt-3.5-turbo
```

### 4. Estrutura do Banco de Dados

```sql
CREATE TABLE document_embeddings (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(1536),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 5. Testando o Sistema de Busca

#### Primeiro, execute a ingestÃ£o:
```bash
python src/ingest.py
```

#### Depois, teste a busca:
```bash
python src/chat.py
```

#### Exemplo de saÃ­da esperada:
```
=== Chat Interativo - Busca Vetorial ===
Digite sua pergunta (ou 'sair' para encerrar):

VocÃª: Qual Ã© o conteÃºdo do documento?

Processando...

Resposta: Com base no contexto fornecido, o documento contÃ©m informaÃ§Ãµes sobre [resposta gerada pelo LLM baseada no conteÃºdo encontrado nos documentos].

==================================================

VocÃª: sair
Encerrando chat...
```

---

## ğŸ“„ ingest.py - Sistema de IngestÃ£o de Documentos

### ğŸš€ Funcionalidades Implementadas

âœ… **Tarefas ConcluÃ­das:**
1. **PDF dividido em chunks** - Chunks de 1000 caracteres com overlap de 150
2. **Cada chunk convertido em embedding** - Usando OpenAI text-embedding-3-small
3. **Vetores armazenados no PostgreSQL** - Com pgVector usando LangChain PGVector

### ğŸ“ˆ Pipeline de Processamento

1. **Carregamento**: PDF Ã© carregado usando PyPDFLoader
2. **Chunking**: Texto dividido em chunks de 1000 caracteres (overlap 150)
3. **Enriquecimento**: Documentos enriquecidos com metadados e IDs Ãºnicos
4. **Embedding**: Cada chunk convertido em vetor usando OpenAI
5. **Armazenamento**: Vetores salvos no PostgreSQL usando LangChain PGVector

### ğŸƒâ€â™‚ï¸ Como Executar

```bash
# Ativar ambiente virtual
source venv/bin/activate

# Executar o pipeline de ingestÃ£o
python src/ingest.py
```

### ğŸ“ Logs de ExecuÃ§Ã£o

O sistema fornece logs detalhados de cada etapa:
- NÃºmero de chunks processados
- Documentos enriquecidos com IDs Ãºnicos
- Status de criaÃ§Ã£o do store PGVector
- Armazenamento dos embeddings

**Exemplo de ExecuÃ§Ã£o:**
```
Processando 67 chunks...

1. Criando store PGVector...
Store PGVector criado com sucesso!

2. Enriqueceendo documentos...
Documento enriquecido: Nome da empresa Faturamento Ano de fundaÃ§Ã£o... (ID: 4bf7fefa-b3b2-4cde-8059-c26ede6198ec)
Documentos enriquecidos: 67

3. Adicionando documentos ao store...
âœ… 67 documentos armazenados com sucesso!

âœ… Pipeline de ingestÃ£o concluÃ­do com sucesso!
ğŸ“Š Dados armazenados no PostgreSQL com pgVector usando LangChain
```

---

## ğŸ” search.py - Sistema de Busca SemÃ¢ntica

### ğŸ¯ Funcionalidades Implementadas
- âœ… **Busca semÃ¢ntica nos embeddings armazenados** - Usando PGVector com similarity_search_with_score
- âœ… **Retrieval de documentos relevantes** - Baseado na pergunta do usuÃ¡rio
- âœ… **SeparaÃ§Ã£o de responsabilidades** - Busca em search.py, LLM em chat.py
- âœ… **ConfiguraÃ§Ã£o automÃ¡tica do store PGVector** - ConexÃ£o com PostgreSQL
- âœ… **Template de prompt estruturado** - Para respostas baseadas em contexto
- âœ… **Tratamento de erros robusto** - Fallbacks sem logs desnecessÃ¡rios
- âœ… **Interface limpa** - Apenas prints essenciais para navegaÃ§Ã£o

### ğŸƒâ€â™‚ï¸ Como Usar

#### FunÃ§Ã£o de Busca: `search_prompt(question)`
```python
from search import search_prompt

# Buscar contexto e formatar prompt
formatted_prompt = search_prompt("Qual Ã© o conteÃºdo do documento?")
print(formatted_prompt)  # Prompt formatado pronto para LLM
```

#### FunÃ§Ã£o de Resposta: `generate_response_with_llm(question)`
```python
from chat import generate_response_with_llm

# Buscar contexto e gerar resposta final com LLM
resposta = generate_response_with_llm("Qual Ã© o conteÃºdo do documento?")
print(resposta)  # Resposta final formatada
```

#### Exemplo de uso no chat:
```python
# No chat interativo, a funÃ§Ã£o jÃ¡ estÃ¡ integrada
from chat import interactive_chat
interactive_chat()
```

### ğŸ“Š CaracterÃ­sticas TÃ©cnicas
- **Embeddings**: OpenAI text-embedding-3-small
- **Busca**: PGVector similarity_search_with_score
- **ConfiguraÃ§Ã£o**: VariÃ¡veis de ambiente (PGVECTOR_URL, PGVECTOR_COLLECTION)
- **Retorno**: Prompt formatado pronto para LLM
- **Responsabilidade**: Apenas busca e formataÃ§Ã£o de contexto

---

## ğŸ’¬ chat.py - Interface de Chat RAG

### ğŸ¯ Funcionalidades Implementadas
- âœ… **Chat interativo completo** - Para testes em tempo real
- âœ… **IntegraÃ§Ã£o com OpenAI LLM** - Usa generate_response_with_llm
- âœ… **GeraÃ§Ã£o de respostas inteligentes** - Baseadas no contexto dos documentos
- âœ… **Tratamento de erros** - ValidaÃ§Ã£o de entrada e fallbacks
- âœ… **Interface limpa** - Apenas mensagens essenciais para navegaÃ§Ã£o
- âœ… **SeparaÃ§Ã£o de responsabilidades** - LLM isolado do sistema de busca

### ğŸƒâ€â™‚ï¸ Como Usar

#### ExecuÃ§Ã£o Direta
```bash
# Executar chat interativo
python src/chat.py
```

#### Uso ProgramÃ¡tico
```python
# Importar e usar diretamente
from chat import interactive_chat
interactive_chat()

# Ou usar apenas a funÃ§Ã£o de geraÃ§Ã£o de resposta
from chat import generate_response_with_llm
resultado = generate_response_with_llm("Sua pergunta aqui")
print(resultado)
```

### ğŸ“ Funcionalidades DisponÃ­veis

1. **`interactive_chat()`** - Chat interativo completo para testes
2. **`generate_response_with_llm(question)`** - GeraÃ§Ã£o de resposta com LLM
3. **IntegraÃ§Ã£o com search.py** - Busca + LLM + resposta final
4. **Respostas inteligentes** - Baseadas no contexto dos documentos

### ğŸ“Š CaracterÃ­sticas TÃ©cnicas
- **LLM**: OpenAI gpt-5-nano (configurÃ¡vel via OPENAI_MODEL)
- **ConfiguraÃ§Ã£o**: VariÃ¡veis de ambiente (OPENAI_MODEL)
- **Responsabilidade**: ExecuÃ§Ã£o do LLM e interface de chat
- **IntegraÃ§Ã£o**: Usa search_prompt do search.py

---

## âš ï¸ ObservaÃ§Ãµes Gerais

- Certifique-se de ter uma API key vÃ¡lida do OpenAI
- O banco PostgreSQL deve estar rodando antes da execuÃ§Ã£o
- A extensÃ£o pgvector deve estar instalada no banco
- Apenas as configuraÃ§Ãµes necessÃ¡rias estÃ£o no arquivo `.env`
- ConfiguraÃ§Ãµes nÃ£o utilizadas foram removidas para manter o projeto limpo